{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1881bae1",
   "metadata": {},
   "source": [
    "### Word Embeddings Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b8654",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to train and save a model that will convert each word in the Yelp reviews corpus to a word vector. Then, once the model training is complete, it can be used in this sentiment analysis project to create a new and imporved dataset for correctly classifying the sentiment of a Yelp review and possibly implementing that into an app review summarizer. This neural network is trained on 20,000 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc6c9c",
   "metadata": {},
   "source": [
    "### Technical Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b050d",
   "metadata": {},
   "source": [
    "The main technical challenges to overcome will be the following:\n",
    "1. Implementing the gensim package correctly so the Word2Vec model can be trained\n",
    "2. Balancing the training time with the accuracy, as the number of weights the model will need to solve for could be 900 million (from about 1.5 million words in the corpus, 300 neurons, and 2 layers)\n",
    "3. Evaluating the performance of this model, comparing it to the performance of the other approaches, and interpreting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a856b56",
   "metadata": {},
   "source": [
    "**We will be using the gensim package to achieve these goals**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574def2",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3b15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.models\n",
    "from gensim import utils\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e5a60",
   "metadata": {},
   "source": [
    "### Load the Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8330c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>isEdited</th>\n",
       "      <th>title</th>\n",
       "      <th>userName</th>\n",
       "      <th>developerResponse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-22 22:44:23</td>\n",
       "      <td>I say it can be fantastic because some people ...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>Yelp can be fantastic</td>\n",
       "      <td>Robg80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-12 22:08:33</td>\n",
       "      <td>Yelp's developers have been spamming false 5-s...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Review botting should not be tolerated!</td>\n",
       "      <td>itsbad):</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-11 18:43:56</td>\n",
       "      <td>I will not be using Yelp ever again. After a t...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Horrible</td>\n",
       "      <td>jennausuwiajdneka</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-09-22 20:35:32</td>\n",
       "      <td>During think tank meetings with other business...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Is yelp fair?</td>\n",
       "      <td>Srepman</td>\n",
       "      <td>{'id': 46973211, 'body': 'Thank you for taking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-13 03:52:13</td>\n",
       "      <td>If I could give this place a 0 star I absolute...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Horrible service</td>\n",
       "      <td>Tsimmons96</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                             review  \\\n",
       "0  2024-11-22 22:44:23  I say it can be fantastic because some people ...   \n",
       "1  2024-12-12 22:08:33  Yelp's developers have been spamming false 5-s...   \n",
       "2  2024-10-11 18:43:56  I will not be using Yelp ever again. After a t...   \n",
       "3  2024-09-22 20:35:32  During think tank meetings with other business...   \n",
       "4  2024-12-13 03:52:13  If I could give this place a 0 star I absolute...   \n",
       "\n",
       "   rating  isEdited                                    title  \\\n",
       "0       5     False                    Yelp can be fantastic   \n",
       "1       1     False  Review botting should not be tolerated!   \n",
       "2       1     False                                 Horrible   \n",
       "3       1     False                            Is yelp fair?   \n",
       "4       1     False                         Horrible service   \n",
       "\n",
       "            userName                                  developerResponse  \n",
       "0             Robg80                                                NaN  \n",
       "1           itsbad):                                                NaN  \n",
       "2  jennausuwiajdneka                                                NaN  \n",
       "3            Srepman  {'id': 46973211, 'body': 'Thank you for taking...  \n",
       "4         Tsimmons96                                                NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"yelp_reviews_v3.csv\")\n",
    "reviews.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a478e2",
   "metadata": {},
   "source": [
    "### Define a generator to feed each review to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c621860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give the tokenized docs (reviews) to the Word2Vec model\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for doc in reviews['review']:\n",
    "            yield utils.simple_preprocess(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a063315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1\n",
      "['say', 'it', 'can', 'be', 'fantastic', 'because', 'some', 'people', 'choose', 'to', 'not', 'post', 'positive', 'reviews', 'and', 'try', 'to', 'disparage', 'businesses', 'with', 'multiple', 'reviews', 'that', 'are', 'intentionally', 'left', 'to', 'hurt', 'the', 'business', 'think', 'yelp', 'has', 'done', 'better', 'job', 'of', 'making', 'sure', 'that', 'these', 'reviews', 'are', 'accurate', 'and', 'also', 'giving', 'the', 'business', 'owners', 'the', 'opportunity', 'to', 'review', 'the', 'reviews', 'in', 'some', 'cases', 'revoking', 'the', 'reviews', 'which', 'believe', 'business', 'owner', 'should', 'have', 'the', 'right', 'to', 'do', 'will', 'always', 'use', 'yelp', 'use', 'it', 'for', 'everything', 'that', 'search', 'for', 'to', 'make', 'sure', 'that', 'get', 'the', 'best', 'quality', 'service', 'the', 'only', 'way', 'yelp', 'piggy', 'get', 'better', 'is', 'if', 'we', 'make', 'it', 'better', 'by', 'posting', 'honest', 'reviews', 'remembering', 'the', 'good', 'service', 'as', 'well', 'as', 'the', 'bad', 'and', 'including', 'magic', 'pictures', 'that', 'take', 'us', 'to', 'the', 'restaurant', 'so', 'we', 'can', 'have', 'the', 'experience']\n",
      "Doc 2\n",
      "['yelp', 'developers', 'have', 'been', 'spamming', 'false', 'star', 'reviews', 'in', 'the', 'app', 'store', 'this', 'company', 'is', 'unethical', 'to', 'say', 'at', 'the', 'very', 'least', 'it', 'would', 'make', 'me', 'extremely', 'happy', 'if', 'there', 'some', 'way', 'to', 'remove', 'said', 'app', 'that', 'was', 'developed', 'by', 'con', 'artists', 'exclusively', 'made', 'to', 'exploit', 'apple', 'platform', 'doing', 'such', 'would', 'force', 'long', 'awaited', 'update', 'for', 'this', 'abysmal', 'app', 'and', 'prevent', 'their', 'scam', 'to', 'continue', 'on', 'your', 'platform', 'yelp', 'is', 'unusable', 'on', 'any', 'other', 'platform', 'browser', 'why', 'would', 'they', 'set', 'up', 'an', 'platform', 'that', 'cannot', 'be', 'viewed', 'online', 'they', 'created', 'yelp', 'just', 'to', 'funnel', 'users', 'to', 'install', 'their', 'glorified', 'reviewing', 'app', 'just', 'want', 'to', 'make', 'it', 'clear', 'that', 'the', 'creators', 'of', 'said', 'app', 'are', 'using', 'the', 'apple', 'app', 'store', 'to', 'pedal', 'downloads', 'by', 'trafficking', 'people', 'from', 'external', 'sources', 'and', 'filling', 'their', 'app', 'store', 'page', 'with', 'biased', 'data', 'information', 'just', 'to', 'manipulate', 'trick', 'unexpecting', 'consumers', 'such', 'businesses', 'practices', 'are', 'crazy', 'unethical', 'please', 'apple', 'don', 'let', 'such', 'shady', 'behavior', 'go', 'unnoticed', 'and', 'unregulated', 'on', 'your', 'platform', 'as', 'regular', 'patron', 'of', 'the', 'appstore', 'refuse', 'to', 'mindlessly', 'scroll', 'through', 'this', 'store', 'page', 'of', 'clear', 'disapproval', 'from', 'the', 'reviews', 'that', 'are', 'clearly', 'sentient', 'of', 'this', 'atrocious', 'and', 'gratuitous', 'leech', 'of', 'business', 'the', 'sheer', 'number', 'of', 'bots', 'on', 'this', 'paige', 'is', 'utterly', 'despicable', 'for', 'those', 'who', 'have', 'conscience', 'unless', 'you', 're', 'yelp', 'developer', 'sorry', 'for', 'the', 'rant']\n",
      "Doc 3\n",
      "['will', 'not', 'be', 'using', 'yelp', 'ever', 'again', 'after', 'terrible', 'encounter', 'with', 'business', 'wrote', 'an', 'honest', 'review', 'about', 'my', 'experience', 'yelp', 'deleted', 'of', 'my', 'accounts', 'because', 'they', 'would', 'not', 'let', 'me', 'write', 'my', 'review', 'due', 'to', 'the', 'business', 'owner', 'being', 'upset', 'if', 'not', 'able', 'to', 'be', 'honest', 'about', 'good', 'and', 'bad', 'experiences', 'then', 'what', 'the', 'point', 'customer', 'service', 'did', 'nothing', 'to', 'help', 'the', 'problem', 'and', 'all', 'of', 'the', 'positive', 'reviews', 'had', 'wrote', 'for', 'small', 'businesses', 'were', 'also', 'deleted', 'with', 'my', 'accounts', 'such', 'shame', 'thought', 'as', 'an', 'app', 'that', 'is', 'centered', 'around', 'honest', 'reviews', 'they', 'would', 'uphold', 'their', 'end', 'of', 'allowing', 'them', 'but', 'no', 'they', 'will', 'side', 'with', 'businesses', 'and', 'delete', 'your', 'yelp', 'account', 'for', 'using', 'the', 'app', 'as', 'it', 'was', 'intended', 'absolutely', 'disgusting', 'have', 'moved', 'onto', 'other', 'apps', 'platforms', 'that', 'respect', 'their', 'user', 'honesty', 'and', 'do', 'not', 'punish', 'them', 'for', 'reviews', 'if', 'business', 'scams', 'people', 'out', 'of', 'their', 'money', 'believe', 'it', 'my', 'responsibility', 'to', 'inform', 'people', 'through', 'public', 'review', 'so', 'that', 'someone', 'else', 'isn', 'victim', 'to', 'it', 'but', 'yelp', 'does', 'not', 'believe', 'in', 'keeping', 'businesses', 'honest', 'so', 'beware', 'yelp', 'will', 'delete', 'your', 'account', 'reviews', 'if', 'you', 'try', 'to', 'warn', 'people', 'about', 'scams', 'do', 'not', 'use']\n"
     ]
    }
   ],
   "source": [
    "# Display the first 3 docs\n",
    "corpus = MyCorpus()\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    if i > 2:\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Doc {i + 1}\")\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4e105",
   "metadata": {},
   "source": [
    "### Train the Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a78d7",
   "metadata": {},
   "source": [
    "See documentation for details: https://radimrehurek.com/gensim/models/word2vec.html     \n",
    "The following code initializes, builds vocabulary, and trains the neural network that we can query for the word embeddings.\n",
    "- Note: we don't need to explicity call .train() here, since training is already done in this step and we don't need to update the neural weights any more. .train() can be used if the model will be trained over multiple sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8411a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    sentences = corpus, # document source\n",
    "    vector_size = 300, # no. of neurons to use/size of the word vectors\n",
    "    window = 5, # context window\n",
    "    sg = 1, # skip-gram model\n",
    "    min_count = 2, # minimum number of times a word has to appear\n",
    "    workers = 2, # parallelization\n",
    "    negative = 10, # negative sampling\n",
    "    seed = 34\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6ed8d",
   "metadata": {},
   "source": [
    "Explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d446224d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.8331689834594727),\n",
       " ('lousy', 0.7492699027061462),\n",
       " ('horrific', 0.7417861223220825),\n",
       " ('awful', 0.7399375438690186),\n",
       " ('crappy', 0.7157835960388184),\n",
       " ('nonexistent', 0.7104875445365906),\n",
       " ('horrendous', 0.7045280933380127),\n",
       " ('prejudice', 0.6949297189712524),\n",
       " ('weak', 0.6941744685173035),\n",
       " ('disgusting', 0.6920444965362549)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = 'horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af183dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yelps', 0.763877272605896),\n",
       " ('religiously', 0.7354929447174072),\n",
       " ('ap', 0.7177634239196777),\n",
       " ('factor', 0.7171481251716614),\n",
       " ('unknown', 0.7122806906700134),\n",
       " ('exclusively', 0.7120497226715088),\n",
       " ('researching', 0.7117982506752014),\n",
       " ('heavily', 0.7111935019493103),\n",
       " ('contributing', 0.711171567440033),\n",
       " ('incentives', 0.7108086943626404)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = 'yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79fdc6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awesome', 0.8157830238342285),\n",
       " ('phenomenal', 0.793861448764801),\n",
       " ('incredible', 0.7890821099281311),\n",
       " ('fantastic', 0.7877715826034546),\n",
       " ('outstanding', 0.7709816694259644),\n",
       " ('fabulous', 0.7696936130523682),\n",
       " ('superb', 0.7609773278236389),\n",
       " ('terrific', 0.7587746977806091),\n",
       " ('wonderful', 0.7521498203277588),\n",
       " ('spectacular', 0.7124101519584656)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = 'amazing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76067e8c",
   "metadata": {},
   "source": [
    "### Save the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea714ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix = 'gensim-model-', delete = False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8c885",
   "metadata": {},
   "source": [
    "Note: this will save the model in a temporary filepath, if you need to load it again, find it in your temporary filepath and move it to the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5590b4",
   "metadata": {},
   "source": [
    "### Create a dataframe using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13e56513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates an average word vector for a document\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vec += model.wv[token].reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b3d16ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.227540</td>\n",
       "      <td>-0.170826</td>\n",
       "      <td>0.115066</td>\n",
       "      <td>0.149463</td>\n",
       "      <td>-0.026197</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>-0.024517</td>\n",
       "      <td>0.025794</td>\n",
       "      <td>0.067489</td>\n",
       "      <td>0.097428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036327</td>\n",
       "      <td>0.172643</td>\n",
       "      <td>0.034766</td>\n",
       "      <td>0.276930</td>\n",
       "      <td>0.098627</td>\n",
       "      <td>-0.086076</td>\n",
       "      <td>-0.055905</td>\n",
       "      <td>-0.056114</td>\n",
       "      <td>-0.002599</td>\n",
       "      <td>0.101394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.199238</td>\n",
       "      <td>-0.200579</td>\n",
       "      <td>0.116510</td>\n",
       "      <td>0.111658</td>\n",
       "      <td>-0.064551</td>\n",
       "      <td>0.020515</td>\n",
       "      <td>-0.017088</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.083793</td>\n",
       "      <td>0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059782</td>\n",
       "      <td>0.178555</td>\n",
       "      <td>0.034858</td>\n",
       "      <td>0.283163</td>\n",
       "      <td>0.098012</td>\n",
       "      <td>-0.086388</td>\n",
       "      <td>-0.065492</td>\n",
       "      <td>-0.055511</td>\n",
       "      <td>-0.080624</td>\n",
       "      <td>0.102011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.229403</td>\n",
       "      <td>-0.180689</td>\n",
       "      <td>0.149343</td>\n",
       "      <td>0.133359</td>\n",
       "      <td>-0.036580</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>-0.011592</td>\n",
       "      <td>-0.025878</td>\n",
       "      <td>0.081141</td>\n",
       "      <td>0.153825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038574</td>\n",
       "      <td>0.177649</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>0.292426</td>\n",
       "      <td>0.114490</td>\n",
       "      <td>-0.116326</td>\n",
       "      <td>-0.075048</td>\n",
       "      <td>-0.072371</td>\n",
       "      <td>-0.070862</td>\n",
       "      <td>0.127622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.237999</td>\n",
       "      <td>-0.193224</td>\n",
       "      <td>0.123863</td>\n",
       "      <td>0.154978</td>\n",
       "      <td>-0.065055</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.086413</td>\n",
       "      <td>0.102024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042580</td>\n",
       "      <td>0.149542</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.302916</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>-0.074216</td>\n",
       "      <td>-0.042382</td>\n",
       "      <td>-0.052624</td>\n",
       "      <td>-0.039711</td>\n",
       "      <td>0.112512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.203110</td>\n",
       "      <td>-0.214482</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.181231</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>-0.013264</td>\n",
       "      <td>-0.050693</td>\n",
       "      <td>0.035647</td>\n",
       "      <td>0.080304</td>\n",
       "      <td>0.143670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005573</td>\n",
       "      <td>0.180557</td>\n",
       "      <td>0.059697</td>\n",
       "      <td>0.240646</td>\n",
       "      <td>0.098397</td>\n",
       "      <td>-0.047990</td>\n",
       "      <td>-0.073707</td>\n",
       "      <td>-0.052928</td>\n",
       "      <td>-0.058238</td>\n",
       "      <td>0.162846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>-0.249139</td>\n",
       "      <td>-0.171055</td>\n",
       "      <td>0.097228</td>\n",
       "      <td>0.166573</td>\n",
       "      <td>-0.020160</td>\n",
       "      <td>0.042528</td>\n",
       "      <td>-0.017260</td>\n",
       "      <td>0.009062</td>\n",
       "      <td>0.058821</td>\n",
       "      <td>0.084938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031552</td>\n",
       "      <td>0.154122</td>\n",
       "      <td>0.021346</td>\n",
       "      <td>0.282789</td>\n",
       "      <td>0.088902</td>\n",
       "      <td>-0.080313</td>\n",
       "      <td>-0.079995</td>\n",
       "      <td>-0.057006</td>\n",
       "      <td>0.013764</td>\n",
       "      <td>0.097351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>-0.215090</td>\n",
       "      <td>-0.221100</td>\n",
       "      <td>0.088470</td>\n",
       "      <td>0.121938</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>-0.050513</td>\n",
       "      <td>0.013177</td>\n",
       "      <td>0.038186</td>\n",
       "      <td>0.057446</td>\n",
       "      <td>0.141945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013193</td>\n",
       "      <td>0.158821</td>\n",
       "      <td>0.045236</td>\n",
       "      <td>0.262819</td>\n",
       "      <td>0.139202</td>\n",
       "      <td>-0.017365</td>\n",
       "      <td>-0.102897</td>\n",
       "      <td>-0.045341</td>\n",
       "      <td>-0.061465</td>\n",
       "      <td>0.188925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>-0.189118</td>\n",
       "      <td>-0.215414</td>\n",
       "      <td>0.072661</td>\n",
       "      <td>0.152371</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.008754</td>\n",
       "      <td>0.008548</td>\n",
       "      <td>0.101564</td>\n",
       "      <td>0.067704</td>\n",
       "      <td>0.090034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012003</td>\n",
       "      <td>0.181716</td>\n",
       "      <td>0.094480</td>\n",
       "      <td>0.284015</td>\n",
       "      <td>0.117345</td>\n",
       "      <td>-0.019341</td>\n",
       "      <td>-0.033495</td>\n",
       "      <td>-0.063061</td>\n",
       "      <td>-0.003370</td>\n",
       "      <td>0.059299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>-0.238020</td>\n",
       "      <td>-0.189898</td>\n",
       "      <td>0.068737</td>\n",
       "      <td>0.205997</td>\n",
       "      <td>-0.052160</td>\n",
       "      <td>0.024170</td>\n",
       "      <td>-0.036865</td>\n",
       "      <td>0.039188</td>\n",
       "      <td>0.100074</td>\n",
       "      <td>0.162745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038671</td>\n",
       "      <td>0.158611</td>\n",
       "      <td>0.056266</td>\n",
       "      <td>0.266769</td>\n",
       "      <td>0.099429</td>\n",
       "      <td>-0.048835</td>\n",
       "      <td>-0.034498</td>\n",
       "      <td>-0.065789</td>\n",
       "      <td>0.064915</td>\n",
       "      <td>0.104764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>-0.214218</td>\n",
       "      <td>-0.114162</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.150878</td>\n",
       "      <td>-0.058581</td>\n",
       "      <td>-0.014961</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.057668</td>\n",
       "      <td>0.083528</td>\n",
       "      <td>0.068772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041696</td>\n",
       "      <td>0.203243</td>\n",
       "      <td>0.058685</td>\n",
       "      <td>0.321522</td>\n",
       "      <td>0.089937</td>\n",
       "      <td>-0.018788</td>\n",
       "      <td>-0.071915</td>\n",
       "      <td>-0.098923</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.035287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.227540 -0.170826  0.115066  0.149463 -0.026197  0.008393 -0.024517   \n",
       "1     -0.199238 -0.200579  0.116510  0.111658 -0.064551  0.020515 -0.017088   \n",
       "2     -0.229403 -0.180689  0.149343  0.133359 -0.036580  0.031731 -0.011592   \n",
       "3     -0.237999 -0.193224  0.123863  0.154978 -0.065055  0.050980  0.001317   \n",
       "4     -0.203110 -0.214482  0.103448  0.181231  0.010507 -0.013264 -0.050693   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995 -0.249139 -0.171055  0.097228  0.166573 -0.020160  0.042528 -0.017260   \n",
       "19996 -0.215090 -0.221100  0.088470  0.121938  0.019137 -0.050513  0.013177   \n",
       "19997 -0.189118 -0.215414  0.072661  0.152371 -0.037037 -0.008754  0.008548   \n",
       "19998 -0.238020 -0.189898  0.068737  0.205997 -0.052160  0.024170 -0.036865   \n",
       "19999 -0.214218 -0.114162  0.003677  0.150878 -0.058581 -0.014961  0.002072   \n",
       "\n",
       "            7         8         9    ...       290       291       292  \\\n",
       "0      0.025794  0.067489  0.097428  ... -0.036327  0.172643  0.034766   \n",
       "1      0.006766  0.083793  0.163523  ... -0.059782  0.178555  0.034858   \n",
       "2     -0.025878  0.081141  0.153825  ... -0.038574  0.177649  0.016944   \n",
       "3      0.003766  0.086413  0.102024  ... -0.042580  0.149542  0.012284   \n",
       "4      0.035647  0.080304  0.143670  ... -0.005573  0.180557  0.059697   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "19995  0.009062  0.058821  0.084938  ... -0.031552  0.154122  0.021346   \n",
       "19996  0.038186  0.057446  0.141945  ... -0.013193  0.158821  0.045236   \n",
       "19997  0.101564  0.067704  0.090034  ... -0.012003  0.181716  0.094480   \n",
       "19998  0.039188  0.100074  0.162745  ... -0.038671  0.158611  0.056266   \n",
       "19999  0.057668  0.083528  0.068772  ... -0.041696  0.203243  0.058685   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "0      0.276930  0.098627 -0.086076 -0.055905 -0.056114 -0.002599  0.101394  \n",
       "1      0.283163  0.098012 -0.086388 -0.065492 -0.055511 -0.080624  0.102011  \n",
       "2      0.292426  0.114490 -0.116326 -0.075048 -0.072371 -0.070862  0.127622  \n",
       "3      0.302916  0.106729 -0.074216 -0.042382 -0.052624 -0.039711  0.112512  \n",
       "4      0.240646  0.098397 -0.047990 -0.073707 -0.052928 -0.058238  0.162846  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19995  0.282789  0.088902 -0.080313 -0.079995 -0.057006  0.013764  0.097351  \n",
       "19996  0.262819  0.139202 -0.017365 -0.102897 -0.045341 -0.061465  0.188925  \n",
       "19997  0.284015  0.117345 -0.019341 -0.033495 -0.063061 -0.003370  0.059299  \n",
       "19998  0.266769  0.099429 -0.048835 -0.034498 -0.065789  0.064915  0.104764  \n",
       "19999  0.321522  0.089937 -0.018788 -0.071915 -0.098923  0.033339  0.035287  \n",
       "\n",
       "[20000 rows x 300 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a dataframe where each row represents the doc's average word vector\n",
    "def calculate_num_docs(corp):\n",
    "    num_docs = 0\n",
    "    for i, doc in enumerate(corp):\n",
    "        num_docs += 1\n",
    "    return num_docs\n",
    "\n",
    "num_docs = calculate_num_docs(corpus)\n",
    "\n",
    "wordvec_arrays = np.zeros((num_docs, 300))\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    wordvec_arrays[i, :] = word_vector(doc, 300)\n",
    "\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "\n",
    "wordvec_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3cede",
   "metadata": {},
   "source": [
    "The above dataframe will be used to build random forest and logistic regression models for sentiment classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
